# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nY36EZ_RXTVhPRpr_7U8HoAa6LMn_piW
"""

!pip install transformers datasets

import pandas as pd
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import nltk
import re
from datasets import load_dataset

nltk.download('punkt')
nltk.download('punkt_tab')

# Load the dataset from Hugging Face
# Change: Replaced Google Drive CSV loading with Hugging Face dataset
ds = load_dataset("Malikeh1375/medical-question-answering-datasets","all-processed")
# Assuming the dataset has a 'train' split; adjust if the split name differs
df = ds['train'].to_pandas()

df = df[['input', 'output']].rename(columns={'input': 'Question', 'output': 'Answer'})

df = df.iloc[:10000]

print(df.head(15))

duplicates = df.duplicated(subset=['Question', 'Answer']).sum()
print(f"Number of duplicates: {duplicates}")
df = df.drop_duplicates(subset=['Question', 'Answer'])

df = df[df['Question'].notna() & df['Answer'].notna()]

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
    text = re.sub(r'[^a-zA-Z0-9\s\.\-\/]', '', text)  # Keep letters, numbers, whitespace, dots, hyphens, slashes
    return text

df['Question'] = df['Question'].apply(clean_text)
df['Answer'] = df['Answer'].apply(clean_text)

# Preprocess text with NLTK tokenization (unchanged)
from nltk.tokenize import word_tokenize

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    return ' '.join(tokens)

df['Question'] = df['Question'].apply(preprocess_text)
df['Answer'] = df['Answer'].apply(preprocess_text)

# Split the dataset into train and validation sets
train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)

tokenizer = T5Tokenizer.from_pretrained('t5-small')

# Dataset class (unchanged)
class QADataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question = str(self.data.iloc[idx]['Question'])
        answer = str(self.data.iloc[idx]['Answer'])

        source = f'answer: {question}'
        target = answer

        source_encoding = self.tokenizer.encode_plus(
            source,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors='pt'
        )

        target_encoding = self.tokenizer.encode_plus(
            target,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors='pt'
        )

        labels = target_encoding['input_ids'][0].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': source_encoding['input_ids'].flatten(),
            'attention_mask': source_encoding['attention_mask'].flatten(),
            'labels': labels
        }

# Create data loaders (unchanged)
max_len = 128
train_dataset = QADataset(train_data, tokenizer, max_len)
val_dataset = QADataset(val_data, tokenizer, max_len)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=4, num_workers=2, pin_memory=True)

# Initialize model and optimizer (unchanged)
model = T5ForConditionalGeneration.from_pretrained('t5-small')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

from torch.cuda.amp import GradScaler, autocast

optimizer = AdamW(model.parameters(), lr=5e-5)
scaler = GradScaler(enabled=(device.type == 'cuda'))

# Training loop (unchanged)
num_epochs = 3
best_val_loss = float('inf')
patience = 2
patience_counter = 0
gradient_accumulation_steps = 4

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for i, batch in enumerate(train_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        with autocast(enabled=(device.type == 'cuda')):
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss / gradient_accumulation_steps

        scaler.scale(loss).backward()

        if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_loader):
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            val_loss += outputs.loss.item()

    avg_val_loss = val_loss / len(val_loader)
    print(f'Validation Loss: {avg_val_loss:.4f}')

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        model.save_pretrained('./best_model')
        tokenizer.save_pretrained('./best_model')
        print("âœ… New best model saved")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("ðŸ›‘ Early stopping")
            break

# Save the final model (unchanged)
model.save_pretrained('./my_t5_model')
tokenizer.save_pretrained('./my_t5_model')

model.save_pretrained('./my_t5_model')
tokenizer.save_pretrained('./my_t5_model')
print("âœ… Final model saved to './my_t5_model'")

# Chatbot response function
def remove_question_prefix(response, question):
    question_no_punct = re.sub(r'[^\w\s]', '', question).strip()
    response_no_punct = re.sub(r'[^\w\s]', '', response).strip()

    if response_no_punct.lower().startswith(question_no_punct.lower()):
        prefix_length = len(question)
        separators = [' ', ' - ', ': ']
        for sep in separators:
            if response.startswith(question + sep):
                return response[len(question) + len(sep):].strip()
        return response[len(question):].strip()
    else:
        return response

def chatbot_response(question):
    model.eval()
    source = f'answer: {question}'
    encoding = tokenizer.encode_plus(
        source,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        add_special_tokens=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_len,
            num_beams=4,
            repetition_penalty=2.5,
            length_penalty=1.0,
            early_stopping=True
        )

    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]
    raw_response = preds[0]
    cleaned_response = remove_question_prefix(raw_response, question)

    if len(cleaned_response.split()) < 10:
        return "Iâ€™m not sure about that. Please consult a healthcare professional for accurate advice."
    else:
        return cleaned_response

# Evaluate on sample validation data (unchanged)
sample_val_data = val_data.sample(n=10, random_state=42)

for index, row in sample_val_data.iterrows():
    question = row['Question']
    expected_answer = row['Answer']
    generated_answer = chatbot_response(question)

    print(f"Question: {question}")
    print(f"Expected Answer: {expected_answer}")
    print(f"Generated Answer: {generated_answer}\n")

# Interactive testing
def interactive_chatbot():
    print("\n=== Medical Chatbot ===")
    print("Enter a medical question (or type 'exit' to quit):")
    while True:
        question = input("Your question: ")
        if question.lower() == 'exit':
            print("Exiting chatbot...")
            break
        if not question.strip():
            print("Please enter a valid question.")
            continue
        response, _ = chatbot_response(question)
        print(f"Chatbot Response: {response}\n")

if __name__ == "__main__":
    interactive_chatbot()

# Load validation data for evaluation
from datasets import load_dataset
def load_validation_data():
    ds = load_dataset("Malikeh1375/medical-question-answering-datasets", "all-processed")
    df = ds['train'].to_pandas()
    df = df[['input', 'output']].rename(columns={'input': 'Question', 'output': 'Answer'})
    df = df.iloc[:10000]
    df = df[df['Question'].notna() & df['Answer'].notna()]

    def clean_text(text):
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^a-zA-Z0-9\s\.\-\/]', '', text)
        return text

    def preprocess_text(text):
        tokens = nltk.word_tokenize(text.lower())
        return ' '.join(tokens)

    df['Question'] = df['Question'].apply(clean_text).apply(preprocess_text)
    df['Answer'] = df['Answer'].apply(clean_text).apply(preprocess_text)

    _, val_data = train_test_split(df, test_size=0.2, random_state=42)
    return val_data

# !pip install rouge-score
# !pip install evaluate

from rouge_score import rouge_scorer
import evaluate  # For METEOR (Hugging Face's evaluate lib)
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  # For BLEU

def evaluate_model(data, num_samples=100):
    print("\n=== Model Evaluation ===")
    sample_data = data.sample(n=min(num_samples, len(data)), random_state=42)

    bleu_scores = []
    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    smoothie = SmoothingFunction().method4

    for index, row in sample_data.iterrows():
        question = row['Question']
        expected_answer = row['Answer']
        generated_answer, is_valid = chatbot_response(question)

        print(f"Question: {question}")
        print(f"Expected Answer: {expected_answer}")
        print(f"Generated Answer: {generated_answer}\n")

        if is_valid:
            reference = [nltk.word_tokenize(expected_answer.lower())]
            hypothesis = nltk.word_tokenize(generated_answer.lower())
            bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)
            bleu_scores.append(bleu_score)

            rouge_result = scorer.score(expected_answer.lower(), generated_answer.lower())
            rouge_scores['rouge1'].append(rouge_result['rouge1'].fmeasure)
            rouge_scores['rouge2'].append(rouge_result['rouge2'].fmeasure)
            rouge_scores['rougeL'].append(rouge_result['rougeL'].fmeasure)

    if bleu_scores:
        avg_bleu = sum(bleu_scores) / len(bleu_scores)
        avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])
        avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])
        avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])

        print(f"Average BLEU Score: {avg_bleu:.4f}")
        print(f"Average ROUGE-1 F1: {avg_rouge1:.4f}")
        print(f"Average ROUGE-2 F1: {avg_rouge2:.4f}")
        print(f"Average ROUGE-L F1: {avg_rougeL:.4f}")

        return {
            'avg_bleu': avg_bleu,
            'avg_rouge1': avg_rouge1,
            'avg_rouge2': avg_rouge2,
            'avg_rougeL': avg_rougeL
        }
    else:
        print("No valid responses for metric calculation.")
        return None

if __name__ == "__main__":
    # Load validation data
    val_data = load_validation_data()
    # Evaluate on validation set
    metrics = evaluate_model(val_data, num_samples=100)

# Download NLTK data
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Verify dataset splits and features
try:
    dataset_name = "Malikeh1375/medical-question-answering-datasets"
    config_name = "all-processed"
    splits = get_dataset_split_names(dataset_name, config_name)
    print(f"Available splits for {dataset_name} ({config_name}): {splits}")
except Exception as e:
    print(f"Error checking dataset splits: {e}")
    exit()

# pip install -U datasets
